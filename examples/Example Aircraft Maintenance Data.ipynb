{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d300869c",
   "metadata": {},
   "source": [
    "# Clustering of Aircraft Maintenance Data\n",
    "\n",
    "This example shows the application of the clustering and tuning process with the example of aircraft maintenance difficulty reports. The test data is sourced from the FAA SDRS: https://sdrs.faa.gov/ and features reports of three different aircraft of the same type but from different airlines. This example will run the full clustering pipeline including clusterng tuning, some general and temporal anlysis. \n",
    "\n",
    "## Import Secton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f1c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from importlib import resources\n",
    "from clustering_pipeline import ClusteringPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from datetime import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb49b502",
   "metadata": {},
   "source": [
    "## Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3305a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PACKAGE_NAME = 'tactic.example_data'\n",
    "RESOURCE_NAME = 'AircraftMaintenance.csv'\n",
    "\n",
    "\n",
    "with resources.files(PACKAGE_NAME).joinpath(RESOURCE_NAME).open('r') as f:\n",
    "    df = pd.read_csv(f, , encoding=\"latin1\")\n",
    "\n",
    "print(f\"Loaded {len(df)} records\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b007c6b2",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "First Initalize a pipeline and load the data. The column relevant for clustering is the column, that includes the written discrepancy report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade0966",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ClusteringPipeline(df, text_column='Discrepancy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c74836",
   "metadata": {},
   "source": [
    "Run the full pipeline. Parameters for tuning are set for this example, however they can be modified as desired. The tsne tuning is False on default as it only serves the purpose of improving the visualization and is not improving clustering or clustering performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d464cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipeline.full_pipeline(\n",
    "        tune_hyperparameters=True,   # enables tuning instead of fixed params\n",
    "        visualize=True,              # produce t-SNE visualization\n",
    "        extract_keywords=True,       # extract keywords after clustering\n",
    "        analyze_topics=True,         # run LDA topic modeling\n",
    "        min_clusters=8,              # minimum cluster count requirement\n",
    "        time_limit=300,              # tuning time in seconds\n",
    "        standardize=True,            # scale data before clustering\n",
    "        tune_tsne=False,             # skip t-SNE tuning\n",
    "        random_state=33,             # ensure reproducibility\n",
    "        num_topics=10,               # number of LDA topics\n",
    "        passes=15,                   # number of LDA passes\n",
    "        tf_top_n=5,                  # number of TF keywords per cluster\n",
    "        yake_top_n=10,               # YAKE initial keywords\n",
    "        yake_final_n=5,              # YAKE final filtered keywords\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da9d99",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "\n",
    "This section of the notebook provides a comprehensive diagnostic report on the text clustering results generated by the pipeline.\n",
    "\n",
    "***\n",
    "\n",
    "### 1. Core Clustering Results & Statistics\n",
    "\n",
    "The analysis establishes the quality and structure of the final clusters:\n",
    "\n",
    "| Feature | Insight |\n",
    "| :--- | :--- |\n",
    "| **Total Clusters** | Reports the final number of distinct, meaningful clusters found. |\n",
    "| **Noise Points** | Calculates the count and **noise ratio** (samples labeled as -1) to assess the effectiveness of HDBSCAN. |\n",
    "| **Best Configuration** | Identifies the optimal **UMAP** (`n_neighbors`, `min_dist`) and **HDBSCAN** (`min_cluster_size`) parameters found during the tuning phase, along with the **Best Score** achieved. |\n",
    "| **Cluster Size** | Prints the size distribution (number of points) for each final cluster. |\n",
    "| **Visualizations** | Generates scatter plots of clusters in **t-SNE** and **UMAP** 2D space to visually confirm separation and density. |\n",
    "\n",
    "***\n",
    "\n",
    "### 2. Performance & Tuning Validation\n",
    "\n",
    "The analysis validates the chosen solution by reviewing the hyperparameter search process:\n",
    "\n",
    "* **Tuning Progress Plots:** Visualizes the evolution of key metrics like the **Davies-Bouldin Index (DBI)** (lower is better) and the **Calinski-Harabasz Index (CHI)** (higher is better) across all iterations.\n",
    "* **Process Metrics:** Tracks the changes in the **Number of Clusters** and **Noise Ratio** during tuning, along with **Cumulative Execution Time**.\n",
    "* **Performance Summary:** Provides a table summarizing the **best, worst, and average** values for all tracked clustering metrics.\n",
    "\n",
    "***\n",
    "\n",
    "### 3. Cluster Interpretation (Keywords)\n",
    "\n",
    "The analysis interprets the thematic focus of each group:\n",
    "\n",
    "* **Semantic Summary:** Prints a concise summary for each cluster, listing the top **YAKE Short keywords** to provide a quick, understandable label (e.g., Cluster 5: \"brake, fluid, leak\").\n",
    "* **Detailed Analysis:** Provides a more extensive view by printing the top 8 YAKE Short keywords and contextualizing them with keywords from YAKE Long.\n",
    "\n",
    "### Purpose and Overview \n",
    "\n",
    "The purpose of this analysis is to obtain an overview of the tuning process and how to interpet the outputs. In this case, the Davis Bouldin Index of the best iteration indicates a clear separation of the clusters while the Callinski-Harabsz Index is not the best one obtained. This indicates that there still is variance inside of the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029caa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def analyze_clustering_results(results):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of clustering results\n",
    "    \"\"\"\n",
    "    # Extract key components\n",
    "    tuning_results = results['clustering']['tuning_results']\n",
    "    clustering_results = results['clustering']['clustering_results']\n",
    "    keywords_df = results['keywords']\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CLUSTERING ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Basic statistics\n",
    "    clusters = clustering_results['clusters']\n",
    "    unique_clusters = np.unique(clusters)\n",
    "    n_clusters = len(unique_clusters) - (1 if -1 in unique_clusters else 0)\n",
    "    n_noise = np.sum(clusters == -1)\n",
    "    noise_ratio = n_noise / len(clusters)\n",
    "    \n",
    "    print(f\"\\n1. CLUSTERING OVERVIEW:\")\n",
    "    print(f\"   • Total samples: {len(clusters)}\")\n",
    "    print(f\"   • Number of clusters: {n_clusters}\")\n",
    "    print(f\"   • Noise points: {n_noise} ({noise_ratio:.1%})\")\n",
    "    print(f\"   • Cluster sizes: {np.bincount(clusters[clusters != -1] + 1)}\")  # +1 to avoid negative indices\n",
    "    \n",
    "    # 2. Best parameters and performance\n",
    "    best_params = tuning_results['best_params']\n",
    "    best_score = tuning_results['best_score']\n",
    "    \n",
    "    print(f\"\\n2. BEST CONFIGURATION (Score: {best_score:.4f}):\")\n",
    "    print(f\"   • UMAP: n_neighbors={best_params['umap']['n_neighbors']}, \"\n",
    "          f\"min_dist={best_params['umap']['min_dist']}, \"\n",
    "          f\"metric='{best_params['umap']['metric']}'\")\n",
    "    print(f\"   • HDBSCAN: min_cluster_size={best_params['hdbscan']['min_cluster_size']}, \"\n",
    "          f\"metric='{best_params['hdbscan']['metric']}', \"\n",
    "          f\"method='{best_params['hdbscan']['cluster_selection_method']}'\")\n",
    "    \n",
    "    # 3. Keyword analysis by cluster - USING YAKE SHORT\n",
    "    print(f\"\\n3. CLUSTER KEYWORDS SUMMARY (YAKE Short):\")\n",
    "    for _, row in keywords_df.iterrows():\n",
    "        cluster_id = row['cluster']\n",
    "        if cluster_id != -1:  # Skip noise\n",
    "            print(f\"   • Cluster {cluster_id}:\")\n",
    "            yake_short_keywords = row['Yake Short'].split(', ')[:3]\n",
    "            print(f\"     - YAKE Short: {', '.join(yake_short_keywords)}\")\n",
    "    \n",
    "    return {\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'noise_ratio': noise_ratio,\n",
    "        'best_score': best_score,\n",
    "        'best_params': best_params\n",
    "    }\n",
    "\n",
    "def plot_tuning_progress(tuning_results):\n",
    "    \"\"\"\n",
    "    Create comprehensive plots showing tuning progress\n",
    "    \"\"\"\n",
    "    history = tuning_results['history']\n",
    "    \n",
    "    # Extract metrics over iterations\n",
    "    iterations = [entry['iteration'] for entry in history]\n",
    "    davies_bouldin = [entry['metrics']['davies_bouldin'] for entry in history]\n",
    "    calinski_harabasz = [entry['metrics']['calinski_harabasz'] for entry in history]\n",
    "    n_clusters = [entry['metrics']['n_clusters'] for entry in history]\n",
    "    noise_ratio = [entry['metrics']['noise_ratio'] for entry in history]\n",
    "    time_elapsed = [entry['time_elapsed'] for entry in history]\n",
    "    \n",
    "    # Handle infinite values for plotting\n",
    "    davies_bouldin_clean = [x if np.isfinite(x) else np.nan for x in davies_bouldin]\n",
    "    calinski_harabasz_clean = [x if np.isfinite(x) else np.nan for x in calinski_harabasz]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = GridSpec(3, 2, figure=fig)\n",
    "    \n",
    "    # Plot 1: Davies-Bouldin Index (lower is better)\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.plot(iterations, davies_bouldin_clean, 'o-', linewidth=2, markersize=4, alpha=0.7)\n",
    "    ax1.set_xlabel('Iteration')\n",
    "    ax1.set_ylabel('Davies-Bouldin Index')\n",
    "    ax1.set_title('Davies-Bouldin Index (Lower = Better)')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark best iteration\n",
    "    best_idx = davies_bouldin_clean.index(min([x for x in davies_bouldin_clean if not np.isnan(x)]))\n",
    "    ax1.plot(iterations[best_idx], davies_bouldin_clean[best_idx], 'ro', markersize=8, label='Best')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot 2: Calinski-Harabasz Index (higher is better)\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.plot(iterations, calinski_harabasz_clean, 'o-', linewidth=2, markersize=4, alpha=0.7, color='green')\n",
    "    ax2.set_xlabel('Iteration')\n",
    "    ax2.set_ylabel('Calinski-Harabasz Index')\n",
    "    ax2.set_title('Calinski-Harabasz Index (Higher = Better)')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Number of clusters and noise ratio\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    ax3_alt = ax3.twinx()\n",
    "    \n",
    "    line1 = ax3.plot(iterations, n_clusters, 'o-', linewidth=2, markersize=4, alpha=0.7, color='purple', label='Number of Clusters')\n",
    "    line2 = ax3_alt.plot(iterations, noise_ratio, 's-', linewidth=2, markersize=4, alpha=0.7, color='orange', label='Noise Ratio')\n",
    "    \n",
    "    ax3.set_xlabel('Iteration')\n",
    "    ax3.set_ylabel('Number of Clusters', color='purple')\n",
    "    ax3_alt.set_ylabel('Noise Ratio', color='orange')\n",
    "    ax3.set_title('Cluster Count vs Noise Ratio')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Combine legends\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax3.legend(lines, labels, loc='upper right')\n",
    "    \n",
    "    # Plot 4: Cumulative time\n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    ax4.plot(iterations, time_elapsed, 'o-', linewidth=2, markersize=4, alpha=0.7, color='red')\n",
    "    ax4.set_xlabel('Iteration')\n",
    "    ax4.set_ylabel('Cumulative Time (seconds)')\n",
    "    ax4.set_title('Cumulative Execution Time')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 5: Parameter distributions\n",
    "    ax5 = fig.add_subplot(gs[2, :])\n",
    "    \n",
    "    # Extract parameter values\n",
    "    umap_nn = [entry['params']['umap']['n_neighbors'] for entry in history]\n",
    "    umap_md = [entry['params']['umap']['min_dist'] for entry in history]\n",
    "    hdbscan_mcs = [entry['params']['hdbscan']['min_cluster_size'] for entry in history]\n",
    "    \n",
    "    # Create parameter evolution plot\n",
    "    param_data = pd.DataFrame({\n",
    "        'Iteration': iterations,\n",
    "        'UMAP n_neighbors': umap_nn,\n",
    "        'UMAP min_dist': umap_md,\n",
    "        'HDBSCAN min_cluster_size': hdbscan_mcs,\n",
    "        'Davies-Bouldin': davies_bouldin_clean\n",
    "    })\n",
    "    \n",
    "    # Normalize for plotting\n",
    "    for col in ['UMAP n_neighbors', 'UMAP min_dist', 'HDBSCAN min_cluster_size']:\n",
    "        param_data[col + '_norm'] = (param_data[col] - param_data[col].min()) / (param_data[col].max() - param_data[col].min())\n",
    "    \n",
    "    # Plot normalized parameters\n",
    "    ax5.plot(iterations, param_data['UMAP n_neighbors_norm'], 'o-', label='UMAP n_neighbors', alpha=0.7)\n",
    "    ax5.plot(iterations, param_data['UMAP min_dist_norm'], 's-', label='UMAP min_dist', alpha=0.7)\n",
    "    ax5.plot(iterations, param_data['HDBSCAN min_cluster_size_norm'], '^-', label='HDBSCAN min_cluster_size', alpha=0.7)\n",
    "    \n",
    "    # Add Davies-Bouldin index (inverted and normalized for comparison)\n",
    "    db_norm = 1 - (param_data['Davies-Bouldin'] - param_data['Davies-Bouldin'].min()) / (param_data['Davies-Bouldin'].max() - param_data['Davies-Bouldin'].min())\n",
    "    ax5.plot(iterations, db_norm, 'd-', linewidth=2, label='Performance (1 - DB norm)', alpha=0.9, color='black')\n",
    "    \n",
    "    ax5.set_xlabel('Iteration')\n",
    "    ax5.set_ylabel('Normalized Values')\n",
    "    ax5.set_title('Parameter Evolution vs Performance')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_cluster_visualizations(clustering_results, keywords_df):\n",
    "    \"\"\"\n",
    "    Create visualizations of the clustering results\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Extract data\n",
    "    tsne_embeddings = clustering_results['tsne_embeddings']\n",
    "    clusters = clustering_results['clusters']\n",
    "    umap_embeddings = clustering_results['umap_embeddings']\n",
    "    \n",
    "    # Plot 1: t-SNE visualization\n",
    "    scatter1 = axes[0, 0].scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], \n",
    "                                 c=clusters, cmap='tab20', alpha=0.7, s=30)\n",
    "    axes[0, 0].set_xlabel('t-SNE Component 1')\n",
    "    axes[0, 0].set_ylabel('t-SNE Component 2')\n",
    "    axes[0, 0].set_title('t-SNE Projection of Clusters')\n",
    "    plt.colorbar(scatter1, ax=axes[0, 0], label='Cluster ID')\n",
    "    \n",
    "    # Plot 2: UMAP first two components\n",
    "    scatter2 = axes[0, 1].scatter(umap_embeddings[:, 0], umap_embeddings[:, 1], \n",
    "                                 c=clusters, cmap='tab20', alpha=0.7, s=30)\n",
    "    axes[0, 1].set_xlabel('UMAP Component 1')\n",
    "    axes[0, 1].set_ylabel('UMAP Component 2')\n",
    "    axes[0, 1].set_title('UMAP Projection (Components 1-2)')\n",
    "    plt.colorbar(scatter2, ax=axes[0, 1], label='Cluster ID')\n",
    "    \n",
    "    # Plot 3: Cluster size distribution\n",
    "    cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "    colors = [plt.cm.tab20(i % 20) for i in range(len(cluster_counts))]\n",
    "    bars = axes[1, 0].bar(cluster_counts.index.astype(str), cluster_counts.values, color=colors, alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Cluster ID')\n",
    "    axes[1, 0].set_ylabel('Number of Points')\n",
    "    axes[1, 0].set_title('Cluster Size Distribution')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    # Plot 4: Top keywords by cluster - USING YAKE SHORT\n",
    "    axes[1, 1].axis('off')\n",
    "    cluster_info = []\n",
    "    for _, row in keywords_df.iterrows():\n",
    "        if row['cluster'] != -1:\n",
    "            # Use YAKE Short keywords instead of TF-IDF\n",
    "            top_keywords = ', '.join(row['Yake Short'].split(', ')[:5])\n",
    "            cluster_info.append(f\"Cluster {row['cluster']}: {top_keywords}\")\n",
    "    \n",
    "    # Display top cluster keywords\n",
    "    text_content = \"\\n\".join(cluster_info[:8])  # Show first 8 clusters\n",
    "    axes[1, 1].text(0.05, 0.95, \"Top Cluster Keywords (YAKE Short):\\n\" + text_content, \n",
    "                   transform=axes[1, 1].transAxes, fontsize=10, \n",
    "                   verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_detailed_keyword_analysis(keywords_df):\n",
    "    \"\"\"\n",
    "    Create a detailed analysis of YAKE Short keywords across clusters\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DETAILED YAKE SHORT KEYWORD ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Filter out noise cluster\n",
    "    meaningful_clusters = keywords_df[keywords_df['cluster'] != -1]\n",
    "    \n",
    "    for _, row in meaningful_clusters.iterrows():\n",
    "        cluster_id = row['cluster']\n",
    "        yake_short_keywords = row['Yake Short'].split(', ')\n",
    "        \n",
    "        print(f\"\\nCluster {cluster_id} - Top YAKE Short Keywords:\")\n",
    "        print(\"  \" + \", \".join(yake_short_keywords[:8]))  # Show top 8 keywords\n",
    "        \n",
    "        # Compare with other methods for context\n",
    "        yake_long_top = row['Yake Long'].split(', ')[:3]\n",
    "        print(f\"  Context (YAKE Long): {', '.join(yake_long_top)}\")\n",
    "\n",
    "def create_performance_summary(tuning_results):\n",
    "    \"\"\"\n",
    "    Create a summary table of performance metrics\n",
    "    \"\"\"\n",
    "    history = tuning_results['history']\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    davies_bouldin_vals = [entry['metrics']['davies_bouldin'] for entry in history if np.isfinite(entry['metrics']['davies_bouldin'])]\n",
    "    calinski_harabasz_vals = [entry['metrics']['calinski_harabasz'] for entry in history if np.isfinite(entry['metrics']['calinski_harabasz'])]\n",
    "    n_clusters_vals = [entry['metrics']['n_clusters'] for entry in history]\n",
    "    noise_ratios = [entry['metrics']['noise_ratio'] for entry in history]\n",
    "    \n",
    "    summary_df = pd.DataFrame({\n",
    "        'Metric': ['Davies-Bouldin Index', 'Calinski-Harabasz Index', 'Number of Clusters', 'Noise Ratio'],\n",
    "        'Best': [min(davies_bouldin_vals), max(calinski_harabasz_vals), \n",
    "                max(n_clusters_vals), min(noise_ratios)],\n",
    "        'Worst': [max(davies_bouldin_vals), min(calinski_harabasz_vals), \n",
    "                 min(n_clusters_vals), max(noise_ratios)],\n",
    "        'Average': [np.mean(davies_bouldin_vals), np.mean(calinski_harabasz_vals),\n",
    "                   np.mean(n_clusters_vals), np.mean(noise_ratios)],\n",
    "        'Std Dev': [np.std(davies_bouldin_vals), np.std(calinski_harabasz_vals),\n",
    "                   np.std(n_clusters_vals), np.std(noise_ratios)]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n4. PERFORMANCE SUMMARY ACROSS ALL ITERATIONS:\")\n",
    "    print(summary_df.to_string(index=False, float_format='%.3f'))\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming your results are stored in a variable called 'results'\n",
    "    # results = your_clustering_output\n",
    "    \n",
    "    # 1. Analyze and print summary\n",
    "    analysis_summary = analyze_clustering_results(results)\n",
    "    \n",
    "    # 2. Create performance summary table\n",
    "    performance_summary = create_performance_summary(results['clustering']['tuning_results'])\n",
    "    \n",
    "    # 3. Detailed YAKE Short keyword analysis\n",
    "    create_detailed_keyword_analysis(results['keywords'])\n",
    "    \n",
    "    # 4. Plot tuning progress\n",
    "    print(\"\\n5. GENERATING TUNING PROGRESS PLOTS...\")\n",
    "    tuning_fig = plot_tuning_progress(results['clustering']['tuning_results'])\n",
    "    \n",
    "    # 5. Plot cluster visualizations\n",
    "    print(\"\\n6. GENERATING CLUSTER VISUALIZATIONS...\")\n",
    "    cluster_fig = plot_cluster_visualizations(\n",
    "        results['clustering']['clustering_results'], \n",
    "        results['keywords']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec59ee",
   "metadata": {},
   "source": [
    "# Temporal Trend Analysis Summary\n",
    "\n",
    "This section provides a deep dive into the **temporal trends** of the text data, examining how the identified clusters (themes/topics) evolve, emerge, persist, and fluctuate over time. The analysis relies on the **Submission Date** of each report.\n",
    "\n",
    "***\n",
    "\n",
    "## 1. Temporal Data Overview & Statistics\n",
    "\n",
    "The analysis begins by prepping the date data and calculating fundamental temporal statistics:\n",
    "\n",
    "* **Data Range:** Reports the minimum and maximum submission dates to establish the **time span** of the dataset.\n",
    "* **Temporal Segmentation:** Segments the data into **Year** and **Month** for granular analysis.\n",
    "* **Cluster Persistence Table:** Generates a statistical summary table for each cluster, detailing its:\n",
    "    * **First and Last Year** of appearance (emergence and decline/persistence).\n",
    "    * **Total Reports** and **Years Active**.\n",
    "    * **Yearly Growth Rate** (from start to end year) to quantify the cluster's momentum.\n",
    "* **Overall Trend Insights:** Calculates and prints high-level insights, including:\n",
    "    * The **Year** with the **most and least cluster diversity** (number of unique clusters).\n",
    "    * The **Overall Growth Rate** of report volume across the entire time frame.\n",
    "    * **New/Emerging Clusters** identified in the most recent year compared to the previous year.\n",
    "    * **Most Persistent Clusters** (those present in every year of the dataset).\n",
    "\n",
    "***\n",
    "\n",
    "## 2. Key Temporal Visualizations\n",
    "\n",
    "A comprehensive set of four plots is generated to visualize cluster dynamics:\n",
    "\n",
    "| Plot Type | X-Axis | Y-Axis | Insight Provided |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Stacked Bar Chart** | Year | % of Reports (Stacked by Cluster) | Shows the **changing dominance** or composition of clusters year-over-year. |\n",
    "| **Yearly Trends Line Plot** | Year | Number of Reports | Tracks the absolute **growth/decline trajectory** of the top 6 largest clusters. |\n",
    "| **Cluster Emergence Timeline** | Date | Cluster ID | Shows the **cumulative growth** of all reports and plots a point for the **first appearance** of each individual cluster. |\n",
    "| **Monthly/Seasonal Patterns** | Month | % of Monthly Reports | Identifies **seasonal fluctuations** by showing the percentage contribution of the top clusters across the 12 months. |\n",
    "\n",
    "***\n",
    "\n",
    "## 3. Advanced Temporal Comparison\n",
    "\n",
    "Two additional plots compare volume and diversity trends:\n",
    "\n",
    "* **Volume vs. Diversity:** A combination plot showing **Total Report Volume** (Bar Chart) and **Cluster Diversity** (Line Plot) by year to assess if growth in volume leads to more diverse problems/themes.\n",
    "* **Yearly Dominance:** A line plot tracking the **Percentage of Yearly Reports** for the top 3 clusters, clearly illustrating shifts in which theme is the most dominant in a given year.\n",
    "\n",
    "***\n",
    "\n",
    "## 4. Correlation Analysis\n",
    "\n",
    "* **Yearly Frequency Correlation:** Calculates and plots a **Heatmap** of the **Pearson correlation** between the yearly frequency counts of all meaningful clusters.\n",
    "* **Strong Correlation Output:** Prints a table listing pairs of clusters whose yearly frequencies are highly correlated ($|r| > 0.5$). This suggests that **these two themes tend to rise and fall together** over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_temporal_trends(results):\n",
    "    \"\"\"\n",
    "    Analyze how clusters are distributed over time using submission dates.\n",
    "    This function preprocesses the data for temporal analysis.\n",
    "    \"\"\"\n",
    "    # Extract dataframe and clustering results\n",
    "    df = results['dataframe']\n",
    "    clusters = results['clustering']['clustering_results']['clusters']\n",
    "    \n",
    "    # Add clusters to dataframe\n",
    "    df = df.copy()\n",
    "    df['Cluster'] = clusters\n",
    "    \n",
    "    # Convert SubmissionDate to datetime\n",
    "    df['SubmissionDate'] = pd.to_datetime(df['SubmissionDate'])\n",
    "    \n",
    "    # Extract year and month for analysis\n",
    "    df['Year'] = df['SubmissionDate'].dt.year\n",
    "    df['Month'] = df['SubmissionDate'].dt.month\n",
    "    df['YearMonth'] = df['SubmissionDate'].dt.to_period('M')\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"TEMPORAL TREND ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Basic temporal statistics\n",
    "    year_range = df['Year'].unique()\n",
    "    print(f\"\\n1. TEMPORAL OVERVIEW:\")\n",
    "    print(f\"   • Data spans from {df['SubmissionDate'].min().strftime('%Y-%m-%d')} to {df['SubmissionDate'].max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   • Years covered: {sorted(year_range)}\")\n",
    "    print(f\"   • Total months: {df['YearMonth'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def plot_temporal_distribution(df, keywords_df):\n",
    "    \"\"\"\n",
    "    Create plots showing cluster distribution over time:\n",
    "    - Stacked bar chart of cluster percentage by year.\n",
    "    - Line plot of yearly trends for top clusters.\n",
    "    - Heatmap of cluster distribution by year.\n",
    "    - Line plot of seasonal (monthly) patterns.\n",
    "    \"\"\"\n",
    "    # Filter out noise points for some analyses\n",
    "    meaningful_df = df[df['Cluster'] != -1]\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Temporal Distribution of Clusters', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Get cluster labels from keywords\n",
    "    cluster_labels = {}\n",
    "    for _, row in keywords_df.iterrows():\n",
    "        cluster_id = row['cluster']\n",
    "        if cluster_id != -1:\n",
    "            top_keywords = ', '.join(row['Yake Short'].split(', ')[:2])\n",
    "            cluster_labels[cluster_id] = f\"{cluster_id}: {top_keywords}\"\n",
    "\n",
    "    # Plot 1: Cluster frequency by year (stacked bar chart)\n",
    "    cluster_year = pd.crosstab(df['Year'], df['Cluster'])\n",
    "    cluster_year_pct = cluster_year.div(cluster_year.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    colors = plt.cm.tab20(np.linspace(0, 1, len(cluster_year_pct.columns)))\n",
    "    cluster_year_pct.plot(kind='bar', stacked=True, ax=axes[0, 0], \n",
    "                          color=[colors[i % len(colors)] for i in range(len(cluster_year_pct.columns))],\n",
    "                          alpha=0.8)\n",
    "    axes[0, 0].set_xlabel('Year')\n",
    "    axes[0, 0].set_ylabel('Percentage of Reports (%)')\n",
    "    axes[0, 0].set_title('Cluster Distribution by Year (Percentage)')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "    # Plot 2: YEARLY trend lines for top clusters\n",
    "    yearly_trends = df.groupby(['Year', 'Cluster']).size().unstack(fill_value=0)\n",
    "    top_clusters = meaningful_df['Cluster'].value_counts().head(6).index\n",
    "    \n",
    "    for cluster in top_clusters:\n",
    "        if cluster in yearly_trends.columns:\n",
    "            axes[0, 1].plot(yearly_trends.index, yearly_trends[cluster], \n",
    "                            marker='o', markersize=6, linewidth=2.5, \n",
    "                            label=cluster_labels.get(cluster, f'Cluster {cluster}'))\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Year')\n",
    "    axes[0, 1].set_ylabel('Number of Reports')\n",
    "    axes[0, 1].set_title('Yearly Trends for Top Clusters')\n",
    "    axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Heatmap of cluster distribution by year\n",
    "    cluster_year_normalized = cluster_year.div(cluster_year.sum(axis=0), axis=1) * 100\n",
    "    \n",
    "    im = axes[1, 0].imshow(cluster_year_normalized.T, cmap='YlOrRd', aspect='auto', alpha=0.8)\n",
    "    axes[1, 0].set_xlabel('Year')\n",
    "    axes[1, 0].set_ylabel('Cluster')\n",
    "    axes[1, 0].set_title('Cluster Distribution Heatmap (Column Normalized)')\n",
    "    \n",
    "    years = sorted(df['Year'].unique())\n",
    "    clusters = sorted([c for c in cluster_year_normalized.columns if c != -1])\n",
    "    axes[1, 0].set_xticks(range(len(years)))\n",
    "    axes[1, 0].set_xticklabels(years)\n",
    "    axes[1, 0].set_yticks(range(len(clusters)))\n",
    "    axes[1, 0].set_yticklabels(clusters)\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[1, 0], label='Percentage (%)')\n",
    "    \n",
    "    # Plot 4: Seasonal patterns (monthly distribution)\n",
    "    monthly_patterns = df.groupby(['Month', 'Cluster']).size().unstack(fill_value=0)\n",
    "    monthly_patterns_pct = monthly_patterns.div(monthly_patterns.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    for cluster in top_clusters[:4]: # Top 4 for clarity\n",
    "        if cluster in monthly_patterns_pct.columns:\n",
    "            axes[1, 1].plot(monthly_patterns_pct.index, monthly_patterns_pct[cluster], \n",
    "                            marker='s', markersize=4, linewidth=2,\n",
    "                            label=cluster_labels.get(cluster, f'Cluster {cluster}'))\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Month')\n",
    "    axes[1, 1].set_ylabel('Percentage of Monthly Reports (%)')\n",
    "    axes[1, 1].set_title('Seasonal Patterns by Cluster')\n",
    "    axes[1, 1].set_xticks(range(1, 13))\n",
    "    axes[1, 1].set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                                'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_temporal_summary_statistics(df, keywords_df):\n",
    "    \"\"\"\n",
    "    Create detailed statistical summary of temporal patterns for each cluster.\n",
    "    \"\"\"\n",
    "    meaningful_df = df[df['Cluster'] != -1]\n",
    "    \n",
    "    print(\"\\n2. TEMPORAL PATTERNS BY CLUSTER:\")\n",
    "    \n",
    "    # Cluster emergence and persistence\n",
    "    cluster_first_occurrence = meaningful_df.groupby('Cluster')['Year'].min()\n",
    "    cluster_last_occurrence = meaningful_df.groupby('Cluster')['Year'].max()\n",
    "    cluster_years_active = meaningful_df.groupby('Cluster')['Year'].nunique()\n",
    "    \n",
    "    temporal_stats = []\n",
    "    for cluster in sorted(meaningful_df['Cluster'].unique()):\n",
    "        cluster_data = meaningful_df[meaningful_df['Cluster'] == cluster]\n",
    "        first_year = cluster_first_occurrence[cluster]\n",
    "        last_year = cluster_last_occurrence[cluster]\n",
    "        years_active = cluster_years_active[cluster]\n",
    "        \n",
    "        # Yearly growth rate calculation\n",
    "        yearly_counts = cluster_data['Year'].value_counts().sort_index()\n",
    "        growth_rate = 0\n",
    "        if len(yearly_counts) > 1 and yearly_counts.iloc[0] > 0:\n",
    "            growth_rate = (yearly_counts.iloc[-1] - yearly_counts.iloc[0]) / yearly_counts.iloc[0] * 100\n",
    "        elif len(yearly_counts) > 1 and yearly_counts.iloc[0] == 0:\n",
    "            growth_rate = np.inf if yearly_counts.iloc[-1] > 0 else 0\n",
    "        \n",
    "        # Get top keywords\n",
    "        keywords_row = keywords_df[keywords_df['cluster'] == cluster]\n",
    "        top_keywords = ', '.join(keywords_row.iloc[0]['Yake Short'].split(', ')[:3]) if not keywords_row.empty else \"N/A\"\n",
    "        \n",
    "        temporal_stats.append({\n",
    "            'Cluster': cluster,\n",
    "            'First Year': first_year,\n",
    "            'Last Year': last_year,\n",
    "            'Years Active': years_active,\n",
    "            'Total Reports': len(cluster_data),\n",
    "            'Growth Rate %': f\"{growth_rate:+.1f}%\" if growth_rate != np.inf else \"+inf%\",\n",
    "            'Top Keywords': top_keywords\n",
    "        })\n",
    "    \n",
    "    temporal_df = pd.DataFrame(temporal_stats)\n",
    "    print(temporal_df.to_string(index=False, max_colwidth=25))\n",
    "    \n",
    "    # Yearly trends analysis\n",
    "    print(f\"\\n3. YEARLY TREND ANALYSIS:\")\n",
    "    yearly_totals = df['Year'].value_counts().sort_index()\n",
    "    print(\"Yearly report counts:\")\n",
    "    for year, count in yearly_totals.items():\n",
    "        meaningful_count = len(meaningful_df[meaningful_df['Year'] == year])\n",
    "        noise_count = count - meaningful_count\n",
    "        print(f\"   {year}: {count} total ({meaningful_count} clustered, {noise_count} noise)\")\n",
    "    \n",
    "    return temporal_df\n",
    "\n",
    "def plot_cluster_growth_trends(df, keywords_df):\n",
    "    \"\"\"\n",
    "    Plot cumulative growth of all reports and the emergence timeline of clusters.\n",
    "    \"\"\"\n",
    "    # Sort by date\n",
    "    df_sorted = df.sort_values('SubmissionDate')\n",
    "    \n",
    "    # Calculate cumulative counts\n",
    "    df_sorted['CumulativeCount'] = range(1, len(df_sorted) + 1)\n",
    "    \n",
    "    # Create cumulative cluster appearance\n",
    "    cluster_first_appearance = df_sorted.groupby('Cluster').first()['CumulativeCount']\n",
    "    \n",
    "    # Plot cumulative growth\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Cumulative reports over time\n",
    "    ax1.plot(df_sorted['SubmissionDate'], df_sorted['CumulativeCount'], \n",
    "             linewidth=2, color='navy', alpha=0.8)\n",
    "    ax1.set_xlabel('Date')\n",
    "    ax1.set_ylabel('Cumulative Number of Reports')\n",
    "    ax1.set_title('Cumulative Growth of Reports')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Plot 2: Cluster emergence timeline\n",
    "    meaningful_clusters = [c for c in cluster_first_appearance.index if c != -1]\n",
    "    cluster_labels = {}\n",
    "    for cluster in meaningful_clusters:\n",
    "        keywords_row = keywords_df[keywords_df['cluster'] == cluster]\n",
    "        if not keywords_row.empty:\n",
    "            label = ', '.join(keywords_row.iloc[0]['Yake Short'].split(', ')[:2])\n",
    "            cluster_labels[cluster] = f\"{cluster}: {label}\"\n",
    "    \n",
    "    for i, cluster in enumerate(meaningful_clusters):\n",
    "        first_date = df_sorted[df_sorted['Cluster'] == cluster].iloc[0]['SubmissionDate']\n",
    "        \n",
    "        ax2.scatter(first_date, i, s=100, alpha=0.7, \n",
    "                    label=cluster_labels.get(cluster, f'Cluster {cluster}'))\n",
    "    \n",
    "    ax2.set_xlabel('Date')\n",
    "    ax2.set_ylabel('Cluster ID')\n",
    "    ax2.set_title('Cluster First Appearance Timeline')\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def plot_yearly_comparison(df, keywords_df):\n",
    "    \"\"\"\n",
    "    Create additional yearly comparison plots for volume, diversity, and cluster dominance.\n",
    "    \"\"\"\n",
    "    meaningful_df = df[df['Cluster'] != -1]\n",
    "    \n",
    "    # Calculate yearly statistics\n",
    "    yearly_stats = meaningful_df.groupby('Year').agg({\n",
    "        'Cluster': ['count', 'nunique']\n",
    "    }).round(2)\n",
    "    yearly_stats.columns = ['Total Reports', 'Cluster Diversity']\n",
    "    \n",
    "    # Yearly cluster composition\n",
    "    yearly_cluster_pct = meaningful_df.groupby(['Year', 'Cluster']).size().unstack(fill_value=0)\n",
    "    yearly_cluster_pct = yearly_cluster_pct.div(yearly_cluster_pct.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Yearly diversity and volume\n",
    "    ax1.bar(yearly_stats.index, yearly_stats['Total Reports'], \n",
    "            alpha=0.7, color='skyblue', label='Total Reports')\n",
    "    ax1_line = ax1.twinx()\n",
    "    ax1_line.plot(yearly_stats.index, yearly_stats['Cluster Diversity'], \n",
    "                  color='red', marker='o', linewidth=2, label='Cluster Diversity')\n",
    "    \n",
    "    ax1.set_xlabel('Year')\n",
    "    ax1.set_ylabel('Number of Reports', color='skyblue')\n",
    "    ax1_line.set_ylabel('Number of Clusters', color='red')\n",
    "    ax1.set_title('Yearly Report Volume vs Cluster Diversity')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax1_line.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper left')\n",
    "    \n",
    "    # Plot 2: Yearly dominance of top clusters\n",
    "    top_clusters = meaningful_df['Cluster'].value_counts().head(3).index\n",
    "    \n",
    "    for cluster in top_clusters:\n",
    "        if cluster in yearly_cluster_pct.columns:\n",
    "            ax2.plot(yearly_cluster_pct.index, yearly_cluster_pct[cluster], \n",
    "                     marker='s', linewidth=2, markersize=6,\n",
    "                     label=f'Cluster {cluster}')\n",
    "    \n",
    "    ax2.set_xlabel('Year')\n",
    "    ax2.set_ylabel('Percentage of Yearly Reports (%)')\n",
    "    ax2.set_title('Yearly Dominance of Top Clusters')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def analyze_cluster_correlations(df):\n",
    "    \"\"\"\n",
    "    Analyze correlations between cluster frequencies over time (yearly).\n",
    "    \"\"\"\n",
    "    # YEARLY cluster frequencies\n",
    "    yearly_clusters = df.groupby(['Year', 'Cluster']).size().unstack(fill_value=0)\n",
    "    \n",
    "    # Calculate correlation matrix, excluding noise cluster -1 if present\n",
    "    meaningful_clusters_freq = yearly_clusters.drop(columns=[-1], errors='ignore')\n",
    "    correlation_matrix = meaningful_clusters_freq.corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Mask upper triangle\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, ax=ax, square=True, fmt='.2f')\n",
    "    ax.set_title('Yearly Cluster Frequency Correlations')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify strongly correlated clusters\n",
    "    strong_correlations = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr) > 0.5: # Threshold for strong correlation\n",
    "                strong_correlations.append({\n",
    "                    'Cluster_A': correlation_matrix.columns[i],\n",
    "                    'Cluster_B': correlation_matrix.columns[j],\n",
    "                    'Correlation': corr\n",
    "                })\n",
    "    \n",
    "    if strong_correlations:\n",
    "        print(f\"\\n4. STRONG YEARLY CORRELATIONS (|r| > 0.5):\")\n",
    "        corr_df = pd.DataFrame(strong_correlations)\n",
    "        print(corr_df.to_string(index=False, float_format='%.3f'))\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n",
    "# Main execution for Temporal Analysis\n",
    "if __name__ == \"__main__\":\n",
    "    # ASSUME 'results' DICTIONARY IS AVAILABLE HERE\n",
    "    # results = your_clustering_output \n",
    "    \n",
    "    # Example placeholder for results (you must replace this with your actual data source)\n",
    "    try:\n",
    "        results = results \n",
    "    except NameError:\n",
    "        print(\"Error: 'results' dictionary not found. Please ensure it is defined and contains 'dataframe', 'clustering', and 'keywords' keys.\")\n",
    "        exit()\n",
    "        \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TEMPORAL ANALYSIS EXECUTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Temporal analysis and preprocessing\n",
    "    df_with_clusters = analyze_temporal_trends(results)\n",
    "    \n",
    "    # 2. Temporal visualizations\n",
    "    print(\"\\nGenerating temporal distribution plots...\")\n",
    "    temporal_fig = plot_temporal_distribution(df_with_clusters, results['keywords'])\n",
    "    \n",
    "    # 3. Additional yearly comparison\n",
    "    print(\"\\nGenerating yearly comparison plots...\")\n",
    "    yearly_fig = plot_yearly_comparison(df_with_clusters, results['keywords'])\n",
    "    \n",
    "    # 4. Temporal statistics\n",
    "    temporal_stats = create_temporal_summary_statistics(df_with_clusters, results['keywords'])\n",
    "    \n",
    "    # 5. Growth trends\n",
    "    print(\"\\nGenerating growth trend plots...\")\n",
    "    growth_fig = plot_cluster_growth_trends(df_with_clusters, results['keywords'])\n",
    "    \n",
    "    # 6. Correlation analysis\n",
    "    print(\"\\nAnalyzing yearly correlations...\")\n",
    "    correlation_matrix = analyze_cluster_correlations(df_with_clusters)\n",
    "    \n",
    "    # 7. Final temporal insights\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"YEARLY TREND INSIGHTS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Calculate some key insights\n",
    "    yearly_cluster_diversity = df_with_clusters.groupby('Year')['Cluster'].nunique()\n",
    "    most_diverse_year = yearly_cluster_diversity.idxmax()\n",
    "    least_diverse_year = yearly_cluster_diversity.idxmin()\n",
    "    \n",
    "    # Yearly growth rates for overall reports\n",
    "    yearly_totals = df_with_clusters['Year'].value_counts().sort_index()\n",
    "    overall_growth = 0\n",
    "    if len(yearly_totals) > 1 and yearly_totals.iloc[0] > 0:\n",
    "        overall_growth = (yearly_totals.iloc[-1] - yearly_totals.iloc[0]) / yearly_totals.iloc[0] * 100\n",
    "    \n",
    "    print(f\"• Year with most cluster diversity: {most_diverse_year} ({yearly_cluster_diversity[most_diverse_year]} clusters)\")\n",
    "    print(f\"• Year with least cluster diversity: {least_diverse_year} ({yearly_cluster_diversity[least_diverse_year]} clusters)\")\n",
    "    print(f\"• Overall growth in reports (start to end year): {overall_growth:+.1f}%\")\n",
    "    \n",
    "    # Identify emerging clusters \n",
    "    recent_year = df_with_clusters['Year'].max()\n",
    "    if recent_year > df_with_clusters['Year'].min():\n",
    "        previous_year = recent_year - 1\n",
    "        recent_clusters = set(df_with_clusters[df_with_clusters['Year'] == recent_year]['Cluster'].unique())\n",
    "        previous_clusters = set(df_with_clusters[df_with_clusters['Year'] == previous_year]['Cluster'].unique())\n",
    "        new_clusters = (recent_clusters - previous_clusters) - {-1} \n",
    "        \n",
    "        if new_clusters:\n",
    "            # Look up keywords for new clusters\n",
    "            new_clusters_labels = []\n",
    "            keywords_df = results['keywords']\n",
    "            for c in sorted(list(new_clusters)):\n",
    "                keywords_row = keywords_df[keywords_df['cluster'] == c]\n",
    "                if not keywords_row.empty:\n",
    "                    top_keyword = keywords_row.iloc[0]['Yake Short'].split(', ')[0]\n",
    "                    new_clusters_labels.append(f\"{c}: {top_keyword}\")\n",
    "                else:\n",
    "                    new_clusters_labels.append(f\"{c}\")\n",
    "            \n",
    "            print(f\"• New clusters emerging in {recent_year}: {new_clusters_labels}\")\n",
    "    \n",
    "    # Most consistent clusters (appear in all years where data exists)\n",
    "    years_present = df_with_clusters['Year'].nunique()\n",
    "    cluster_persistence = df_with_clusters.groupby('Cluster')['Year'].nunique()\n",
    "    most_persistent = cluster_persistence[cluster_persistence == years_present].index.tolist()\n",
    "    print(f\"• Most persistent clusters (appear in all {years_present} years): {most_persistent}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
